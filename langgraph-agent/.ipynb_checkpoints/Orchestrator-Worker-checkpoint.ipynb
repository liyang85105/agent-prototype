{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc872ea4-7192-40a5-aac8-cec858b6f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_llm import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ae4ec5-8f40-4000-8c4a-ad64dfb7adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83bb2b2-8649-425f-9682-44feb9706717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78e7f1e-8870-4af5-81db-a53d723cabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.constants import Send\n",
    "\n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write a report section.\"),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c5bb29-7451-48ce-8120-030c2573c563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "    \n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeb5c0e-a4e9-4633-a86a-6be950b5e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1. Introduction to LLM Scaling Laws\n",
       "\n",
       "The rapid advancements in artificial intelligence over the past decade have been significantly propelled by the emergence of Large Language Models (LLMs). These sophisticated AI models, trained on vast datasets of text and code, have demonstrated unprecedented capabilities in understanding, generating, and manipulating human language, leading to breakthroughs in natural language processing, content creation, and human-computer interaction. Their impressive performance, often characterized by emergent abilities not seen in smaller models, has reshaped the landscape of AI research and application.\n",
       "\n",
       "Central to the remarkable progress and predictable improvement of LLMs is the concept of **scaling laws**. These empirical observations describe predictable relationships between a model's performance (e.g., loss, accuracy, or specific task metrics) and the resources invested in its training. Key resources typically include the number of model parameters, the size of the training dataset, and the computational budget (measured in floating-point operations, or FLOPs). Scaling laws suggest that, under certain conditions, simply increasing these resources leads to measurable and often predictable improvements in model quality, providing a powerful framework for understanding and guiding the development of increasingly capable AI systems.\n",
       "\n",
       "The significance of scaling laws cannot be overstated. They offer crucial guidance for researchers and engineers, enabling the prediction of future model performance, optimizing resource allocation for training, and fostering a deeper understanding of the underlying mechanisms driving AI progress. By transforming the development of large AI models from an intuitive art into a more predictable science, scaling laws have become a cornerstone of modern AI research, driving the current wave of innovation and the pursuit of ever-larger and more powerful models.\n",
       "\n",
       "This report aims to provide a comprehensive overview of LLM scaling laws. We will delve into their foundational principles, explore key empirical findings and theoretical frameworks that underpin them, and discuss the methodologies used to discover and validate these relationships. Furthermore, we will examine their practical implications for model design, training, and deployment, while also addressing current limitations, open questions, and future research directions in this critical area of artificial intelligence. By illuminating the critical role scaling laws play in the evolution of large language models, we seek to enhance understanding of their potential to shape the future of AI.\n",
       "\n",
       "---\n",
       "\n",
       "## Fundamentals of LLM Scaling\n",
       "\n",
       "The remarkable capabilities of Large Language Models (LLMs) are intrinsically linked to their scale. This section elucidates the three fundamental components that govern LLM scaling: model size (parameters), dataset size (tokens), and computational budget (FLOPs). Understanding their individual roles and synergistic interactions is crucial for optimizing model performance and resource allocation.\n",
       "\n",
       "### Model Size (Parameters)\n",
       "\n",
       "Model size refers to the number of trainable parameters within the neural network architecture. These parameters, primarily weights and biases, represent the knowledge and patterns the model learns from its training data. A larger number of parameters generally allows the model to capture more complex relationships, store more information, and exhibit greater capacity for learning intricate linguistic structures. For instance, increasing parameters enables models to handle longer contexts, understand nuanced semantics, and generate more coherent and diverse outputs. However, increasing parameters also directly correlates with higher computational demands for both training and inference, and a greater risk of overfitting if not adequately trained on sufficient data.\n",
       "\n",
       "### Dataset Size (Tokens)\n",
       "\n",
       "Dataset size is quantified by the total number of tokens (e.g., words, sub-word units, or characters) the model is exposed to during its pre-training phase. The quality and quantity of this data are paramount. A larger, more diverse, and higher-quality dataset provides the model with a richer source of information to learn from, enabling it to generalize better, reduce biases, and improve factual accuracy. Scaling the dataset size is critical for preventing underfitting, enhancing the model's ability to follow instructions, and generating coherent, contextually relevant text. It's often observed that larger models require proportionally larger datasets to fully realize their potential, as insufficient data can lead to underutilization of the model's vast parameter space.\n",
       "\n",
       "### Computational Budget (FLOPs)\n",
       "\n",
       "Computational budget, typically measured in Floating Point Operations (FLOPs), represents the total amount of computation expended during the model's training process. FLOPs serve as a proxy for the energy, time, and financial resources required to train an LLM. Training involves numerous matrix multiplications and other operations, each contributing to the total FLOP count. A larger computational budget allows for training larger models, training models for more epochs, or training on larger datasets. It is the ultimate constraint that dictates the feasible scale of both model and data. Optimizing the allocation of this budget across model size and dataset size is a central challenge in LLM development, as different allocations can lead to vastly different performance outcomes for the same total FLOPs.\n",
       "\n",
       "### Interplay and Scaling Laws\n",
       "\n",
       "These three components are not independent but are deeply intertwined. The optimal performance of an LLM is achieved not by maximizing one component in isolation, but by finding the right balance among them given a fixed computational budget. Recent research, particularly on \"scaling laws\" (e.g., Kaplan et al., 2020; Hoffmann et al., 2022), has provided empirical insights into how performance scales with increases in parameters, data, and FLOPs. These laws suggest that for a given computational budget, there is an optimal allocation between model size and dataset size. For instance, the Chinchilla scaling laws demonstrated that many previous large models were \"undertrained\" relative to their size, meaning they could have benefited from significantly more training data for the same FLOPs, leading to better performance.\n",
       "\n",
       "In essence, effective LLM scaling is a multi-dimensional optimization problem. A comprehensive understanding of model parameters, training data volume, and the computational resources available is fundamental to designing, training, and deploying high-performing and efficient large language models.\n",
       "\n",
       "---\n",
       "\n",
       "## Chinchilla Scaling Laws\n",
       "\n",
       "The Chinchilla scaling laws, introduced by Hoffmann et al. (DeepMind, 2022), represent a pivotal advancement in the understanding of large language model (LLM) training dynamics. Prior to this work, the prevailing strategy for improving LLM performance often prioritized increasing model size (number of parameters, N) over the amount of training data (number of tokens, D). The Chinchilla study fundamentally challenged this paradigm by demonstrating that for optimal performance at a given compute budget (C), both model size and the number of training tokens must be scaled proportionally, with a significantly higher emphasis on data than previously assumed.\n",
       "\n",
       "**Core Principle and Optimal Trade-offs:**\n",
       "\n",
       "The central finding of the Chinchilla research is that many large language models trained before 2022 were \"under-trained\" relative to their size. This means that for the same computational cost, a smaller model trained on substantially more data could achieve equivalent or superior performance. The researchers empirically derived power-law relationships between N, D, and C, showing that the test loss of a language model scales predictably with these variables.\n",
       "\n",
       "Crucially, for a fixed compute budget, the Chinchilla laws dictate an optimal trade-off between N and D that minimizes the final loss. Their analysis revealed that the total compute budget (C) scales approximately as C â‰ˆ 6ND, where 6 represents the number of floating-point operations (FLOPs) per token per parameter. This relationship implies that for a given C, there is an optimal combination of N and D that yields the lowest possible loss. Their key finding was that the optimal number of training tokens for a given compute budget is significantly higher than previously assumed, leading to the conclusion that many large models were compute-optimal at smaller sizes with more data. For instance, they found that a 70-billion parameter model trained on 1.4 trillion tokens (Chinchilla) outperformed a 280-billion parameter model trained on 300 billion tokens (Gopher), despite both consuming roughly the same compute budget.\n",
       "\n",
       "**Implications for LLM Development:**\n",
       "\n",
       "The Chinchilla scaling laws have profound implications for the efficient development and deployment of LLMs:\n",
       "\n",
       "1.  **Resource Optimization:** They provide a clear framework for allocating computational resources more effectively. Instead of blindly pursuing ever-larger models, practitioners are guided to invest in larger, higher-quality datasets and longer training runs for moderately sized models.\n",
       "2.  **Improved Performance-to-Cost Ratio:** By optimizing the N-D trade-off, developers can achieve better performance for a given compute budget, or achieve comparable performance with significantly less compute, leading to more cost-effective and energy-efficient models.\n",
       "3.  **Shift in Design Philosophy:** The findings have encouraged a shift from \"model-centric\" scaling to \"data-centric\" scaling, emphasizing the critical role of data quantity and quality in achieving state-of-the-art performance.\n",
       "4.  **Guidance for Future Models:** The empirical relationships derived from Chinchilla serve as a foundational guideline for designing the next generation of LLMs, ensuring that they are \"compute-optimal\" from the outset.\n",
       "\n",
       "**Limitations and Nuances:**\n",
       "\n",
       "While highly influential, it is important to acknowledge certain nuances and potential limitations:\n",
       "\n",
       "*   **Empirical Nature:** The laws are derived empirically and may not hold universally across all model architectures, training objectives, or highly specialized data distributions.\n",
       "*   **Data Quality:** The laws primarily focus on the *quantity* of data. The *quality* of training data remains a critical, often unquantified, factor that can significantly impact model performance regardless of scale.\n",
       "*   **Beyond Compute:** The Chinchilla laws primarily optimize for training compute. Other factors, such as inference costs, memory constraints, and specific downstream task performance, also influence the optimal model design and may lead to different trade-offs.\n",
       "*   **Diminishing Returns:** While the laws suggest continued benefits from scaling, there are practical limits and potential diminishing returns at extreme scales, both in terms of data availability and the ability to extract further knowledge from it.\n",
       "\n",
       "In summary, the Chinchilla scaling laws represent a pivotal advancement in understanding LLM training dynamics, advocating for a balanced and data-intensive approach to model development. By optimizing the trade-off between model size and data for a given compute budget, they enable the creation of more efficient, powerful, and resource-conscious large language models, fundamentally reshaping the landscape of AI research and development.\n",
       "\n",
       "---\n",
       "\n",
       "### Kaplan Scaling Laws\n",
       "\n",
       "The Kaplan Scaling Laws, proposed by OpenAI in their seminal 2020 paper \"Scaling Laws for Neural Language Models\" by Kaplan et al., represent a foundational empirical understanding of how the performance of large language models (LLMs) scales with key computational resources. These laws were among the first to systematically demonstrate the predictable, power-law relationships between model performance and the resources invested in their training.\n",
       "\n",
       "At their core, the Kaplan Scaling Laws highlight that as compute, model parameters, and dataset size increase, the cross-entropy loss (a common measure of model performance, where lower is better) decreases in a consistent, power-law fashion. This implies that performance improvements, while exhibiting diminishing returns, continue steadily as resources are scaled up, rather than plateauing abruptly.\n",
       "\n",
       "Specifically, the research identified three primary dimensions of scaling:\n",
       "\n",
       "1.  **Compute (C):** Performance was found to improve as a power law of the total compute (measured in FLOPs) used for training. This suggests that simply throwing more computational power at a model, even with fixed model size and data, leads to better results.\n",
       "2.  **Parameters (N):** Increasing the number of model parameters (i.e., the model's size) also led to predictable improvements in performance. Larger models, even when trained for the same number of tokens, were shown to achieve lower loss, indicating a greater capacity to learn complex patterns and representations.\n",
       "3.  **Data (D):** The quantity of training data was identified as another critical factor. As the dataset size increased, models consistently achieved better performance, demonstrating the importance of exposing models to a vast and diverse range of information.\n",
       "\n",
       "A key insight from Kaplan et al. was that these factors are not independent but interact. For a given compute budget, there exists an optimal balance between model size and the amount of data it is trained on. While the initial findings suggested that larger models were generally more \"sample efficient\" (meaning they could achieve better performance with less data than smaller models, given sufficient compute), the overarching message was the consistent and predictable nature of performance gains through scaling.\n",
       "\n",
       "The Kaplan Scaling Laws provided a crucial empirical framework that shifted the focus of AI research from architectural tweaks to the strategic allocation of resources. They offered a predictive roadmap for designing and training increasingly capable large language models, demonstrating that significant performance gains could be reliably achieved by simply scaling up the fundamental components of the training process. This understanding has been instrumental in guiding the development of subsequent generations of LLMs and continues to inform research into efficient scaling strategies.\n",
       "\n",
       "---\n",
       "\n",
       "## Other Scaling Observations and Phenomena\n",
       "\n",
       "While scaling laws, such as those proposed by Chinchilla and Kaplan, provide quantitative frameworks for predicting model performance based on compute, parameter count, and dataset size, the landscape of large language models (LLMs) reveals a richer, more complex set of scaling observations and phenomena. These go beyond simple numerical extrapolation, highlighting qualitative shifts and emergent behaviors that manifest as models grow.\n",
       "\n",
       "### Emergent Abilities\n",
       "\n",
       "One of the most striking observations in LLM scaling is the appearance of \"emergent abilities.\" These are capabilities that are not present in smaller models and cannot be easily predicted by extrapolating performance from smaller scales. Instead, they \"emerge\" suddenly and non-linearly once a model reaches a certain critical size or training budget. Examples include:\n",
       "\n",
       "*   **In-context learning:** The ability to learn new tasks or adapt to new instructions purely from examples provided within the prompt, without explicit fine-tuning.\n",
       "*   **Chain-of-thought reasoning:** The capacity to break down complex problems into intermediate steps, leading to more accurate and interpretable solutions, particularly in arithmetic, symbolic reasoning, and logical deduction.\n",
       "*   **Complex problem-solving:** Performance on tasks requiring multi-step reasoning, code generation, or scientific problem-solving often shows a sharp increase at specific scales.\n",
       "*   **Tool use:** The ability to effectively integrate and utilize external tools (e.g., search engines, calculators, code interpreters) to augment their capabilities.\n",
       "\n",
       "The unpredictability of these emergent abilities poses significant challenges for forecasting future model capabilities and necessitates new evaluation methodologies that can capture these qualitative leaps.\n",
       "\n",
       "### Phase Changes in Model Behavior\n",
       "\n",
       "Closely related to emergent abilities are observations of \"phase changes\" in model behavior. Analogous to physical phenomena like water transitioning from liquid to ice, these refer to sudden, discontinuous shifts in a model's performance or internal representations. Instead of a smooth, continuous improvement, performance on certain tasks can jump dramatically once a specific scale threshold is crossed.\n",
       "\n",
       "These phase changes suggest that as models accumulate more parameters and are exposed to vast amounts of data, they may develop more sophisticated internal representations or acquire new cognitive mechanisms that fundamentally alter their processing capabilities. The exact mechanisms driving these phase changes are still an active area of research, but they underscore the non-linear nature of scaling and the potential for unexpected breakthroughs.\n",
       "\n",
       "### Beyond Pure Compute and Parameter Scaling\n",
       "\n",
       "Beyond the core scaling laws, other factors have been observed to significantly influence model capabilities and efficiency as scale increases:\n",
       "\n",
       "*   **Data Quality and Diversity:** While quantity is crucial, the *quality* and *diversity* of training data become increasingly paramount at larger scales. High-quality, curated datasets can unlock capabilities that sheer volume of lower-quality data cannot, leading to \"data scaling laws\" that emphasize the importance of data efficiency.\n",
       "*   **Architectural Innovations:** Novel architectural designs, such as Mixture-of-Experts (MoE) models, enable scaling to vastly larger parameter counts while maintaining or improving training and inference efficiency. These innovations allow for different forms of \"scaling\" that are not solely about increasing dense parameter counts.\n",
       "*   **Optimization and Training Stability:** As models grow, training stability becomes a critical challenge. Advanced optimization techniques, learning rate schedules, and regularization methods are essential to successfully train models at extreme scales and prevent divergence or performance degradation.\n",
       "*   **Alignment and Safety Challenges:** The challenges associated with aligning LLMs with human values and ensuring their safe and ethical deployment also scale with model capability. More powerful models can exhibit more complex and subtle undesirable behaviors, necessitating sophisticated alignment techniques that evolve with model scale.\n",
       "\n",
       "In summary, while quantitative scaling laws provide a foundational understanding, the observed phenomena of emergent abilities and phase changes highlight that scaling LLMs is not merely about \"more,\" but about qualitative transformations in capability. Furthermore, the interplay of data quality, architectural innovation, and training methodologies significantly influences the trajectory and nature of these scaling advancements.\n",
       "\n",
       "---\n",
       "\n",
       "## Implications for LLM Development\n",
       "\n",
       "Scaling laws have emerged as a foundational principle guiding the development of large language models (LLMs), offering critical insights into the relationship between computational resources, data, model size, and performance. Their practical implications span the entire lifecycle of an LLM, from initial design choices to training methodologies and eventual deployment strategies, fundamentally reshaping resource allocation and efficiency considerations.\n",
       "\n",
       "**1. Optimal Resource Allocation and the Chinchilla Paradigm:**\n",
       "Perhaps the most significant practical implication is the shift in understanding optimal resource allocation. Early scaling efforts often prioritized increasing model parameters. However, recent scaling laws, notably those derived from the \"Chinchilla\" model, demonstrate that for a given compute budget, models are significantly *under-trained* on data. This implies that future LLMs, to achieve optimal performance, will require a more balanced allocation across compute, data, and model parameters. Developers are now incentivized to:\n",
       "*   **Invest more heavily in data curation:** Prioritizing the collection and meticulous cleaning of vast, high-quality datasets becomes paramount, as data quantity and quality directly impact the efficiency of compute utilization.\n",
       "*   **Rethink parameter-to-token ratios:** Instead of simply building larger models, the focus shifts to training models with a more optimal number of parameters on significantly more tokens, leading to better performance for the same computational cost.\n",
       "*   **Strategic compute budgeting:** Compute resources must be allocated not just for model size, but for the extensive training required to fully leverage the model's capacity on an expanded dataset.\n",
       "\n",
       "**2. Strategic Model Design and Architecture:**\n",
       "Scaling laws directly inform architectural decisions and the overall design philosophy of LLMs.\n",
       "*   **Efficiency over sheer size:** While larger models generally perform better, the diminishing returns observed in some scaling regimes encourage the exploration of more efficient architectures. This includes investigating sparse models, Mixture-of-Experts (MoE) architectures, or novel transformer variants that can achieve high performance with lower active parameter counts during inference or more efficient training.\n",
       "*   **Tailored model sizes:** Understanding scaling laws allows developers to design models specifically for target performance levels and deployment environments. For instance, a model intended for edge deployment will have different scaling considerations than one designed for a large cloud-based service, leading to a more deliberate choice of parameter count and training data.\n",
       "*   **Predictive performance modeling:** Scaling laws enable developers to predict the approximate performance of a model given certain resource constraints, allowing for more informed decisions before embarking on costly training runs.\n",
       "\n",
       "**3. Training Paradigm Shifts:**\n",
       "The implications for the training phase are profound, demanding significant adjustments in infrastructure and methodology.\n",
       "*   **Massive compute requirements:** The need to train models on vastly more tokens translates directly into an unprecedented demand for computational resources (GPUs, TPUs). This necessitates substantial investment in specialized hardware and robust, scalable distributed training infrastructure.\n",
       "*   **Extended training durations:** Training optimally scaled models can take significantly longer, requiring sophisticated job scheduling, fault tolerance mechanisms, and continuous monitoring to manage multi-month training runs.\n",
       "*   **Energy consumption:** The increased compute and training time exacerbate concerns about energy consumption and environmental impact, pushing for research into more energy-efficient hardware and algorithms.\n",
       "*   **Data pipeline optimization:** The emphasis on data volume and quality necessitates highly optimized data loading, preprocessing, and augmentation pipelines to feed the models efficiently during training.\n",
       "\n",
       "**4. Deployment and Inference Optimization:**\n",
       "Once trained, the practical deployment of LLMs is heavily influenced by scaling considerations, particularly regarding efficiency and accessibility.\n",
       "*   **Inference cost reduction:** Larger models, even if optimally trained, incur substantial inference costs in terms of memory footprint, latency, and throughput. This drives the development and adoption of techniques like:\n",
       "    *   **Quantization:** Reducing the precision of model weights (e.g., from FP32 to FP16, INT8, or even INT4) to decrease memory usage and speed up computation.\n",
       "    *   **Pruning:** Removing redundant connections or neurons from the model without significant performance degradation.\n",
       "    *   **Knowledge Distillation:** Training a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model, enabling faster and cheaper inference.\n",
       "    *   **Efficient serving frameworks:** Developing specialized software and hardware accelerators for LLM inference.\n",
       "*   **Accessibility and democratization:** By enabling the creation of smaller, yet highly capable models through techniques like distillation or by finding more efficient scaling paths, LLMs can become more accessible for deployment on a wider range of hardware, from mobile devices to local servers, reducing reliance on massive cloud infrastructure.\n",
       "\n",
       "In essence, scaling laws are not just theoretical constructs; they serve as a practical guide for optimizing resource allocation, designing efficient architectures, and navigating the complex trade-offs inherent in the pursuit of increasingly capable and accessible large language models. They underscore that continued progress in LLM development is as much about intelligent resource management and efficiency as it is about raw computational power.\n",
       "\n",
       "---\n",
       "\n",
       "## Challenges and Future Directions\n",
       "\n",
       "While current Large Language Model (LLM) scaling laws have provided invaluable insights into the relationship between computational resources, model size, data quantity, and performance, they also present significant challenges and open questions that necessitate further research and development. Addressing these limitations is crucial for the responsible and effective advancement of AI.\n",
       "\n",
       "### Current Challenges and Limitations\n",
       "\n",
       "1.  **Impact of Architecture:**\n",
       "    Current scaling laws often treat architecture as a fixed variable or assume universal applicability across diverse designs. However, the specific architectural choices (e.g., attention mechanisms, normalization layers, activation functions, mixture-of-experts (MoE) layers) can profoundly influence how efficiently and effectively a model scales. For instance, while Transformers have dominated, it's unclear if their scaling properties are universally optimal or if novel architectures could achieve similar performance with fewer resources or different scaling exponents. The interplay between architectural innovation and scaling behavior remains an underexplored area.\n",
       "\n",
       "2.  **Data Quality, Diversity, and Curation:**\n",
       "    The quality, diversity, and curation of training data are increasingly recognized as critical factors that can either amplify or diminish the benefits of scale. Scaling laws typically assume homogeneous data quality, yet real-world datasets are often noisy, biased, and contain redundancies. Data contamination (e.g., test set leakage into training data) can artificially inflate performance metrics, making true scaling effects difficult to discern. Furthermore, the optimal *mix* of data types (e.g., code, text, multimodal) and the impact of data freshness on scaling are not yet fully understood. Simply increasing data quantity without regard for quality or relevance may lead to diminishing returns or even negative consequences like increased bias.\n",
       "\n",
       "3.  **Interpretability and Theoretical Understanding:**\n",
       "    Despite their predictive power, current scaling laws are largely empirical observations, offering limited mechanistic insight into *why* LLMs exhibit improved performance with scale. The \"black box\" nature of these models makes it challenging to understand the underlying computational principles that govern their emergent abilities. A lack of interpretability hinders our ability to diagnose failures, ensure safety, and theoretically ground the observed scaling phenomena. Moving beyond empirical fits to a more fundamental theoretical understanding of how information is processed, represented, and generalized within large neural networks as they scale is a significant open question.\n",
       "\n",
       "4.  **Beyond Loss-Centric Metrics:**\n",
       "    Most established scaling laws primarily focus on the reduction of perplexity or cross-entropy loss. However, real-world utility extends far beyond these metrics to include aspects like factual accuracy, reasoning capabilities, robustness to adversarial attacks, safety, alignment with human values, and the mitigation of harmful biases. \"Emergent abilities\" that appear non-linearly with scale are particularly challenging to predict or quantify using current loss-based scaling laws. Developing scaling laws that account for these complex, qualitative, and often subjective metrics is a major challenge.\n",
       "\n",
       "5.  **Computational Cost and Sustainability:**\n",
       "    The immense computational resources required to train and deploy ever-larger LLMs raise significant concerns regarding environmental sustainability and accessibility. The energy consumption and carbon footprint associated with training models with trillions of parameters are substantial. Moreover, the escalating costs create a high barrier to entry, concentrating advanced AI development in the hands of a few well-resourced organizations, potentially stifling broader innovation and diverse perspectives.\n",
       "\n",
       "### Future Directions\n",
       "\n",
       "1.  **Towards More Nuanced and Holistic Scaling Laws:**\n",
       "    Future research should aim to develop more sophisticated scaling laws that incorporate architectural parameters, data characteristics (e.g., quality scores, diversity metrics, data mixture ratios), and task-specific complexities. This could involve multi-objective scaling laws that optimize for a combination of loss, safety, and alignment metrics, rather than just perplexity.\n",
       "\n",
       "2.  **Enhancing Interpretability and Theoretical Foundations:**\n",
       "    A critical direction involves developing theoretical frameworks that explain *why* scaling works, moving beyond empirical observations. This includes research into mechanistic interpretability to understand how specific model components contribute to emergent abilities at scale. Bridging the gap between empirical scaling laws and fundamental theories of learning in large neural networks will be paramount.\n",
       "\n",
       "3.  **Data-Centric AI for Scaling:**\n",
       "    Future efforts will focus on optimizing data strategies rather than solely increasing data volume. This includes research into active learning, synthetic data generation, data distillation, and intelligent data curation techniques to maximize the impact of each training token. Developing methods to quantify and improve data quality will be essential for efficient and effective scaling.\n",
       "\n",
       "4.  **Efficiency and Sustainability in Scaling:**\n",
       "    Research into parameter-efficient scaling methods (e.g., sparse models, conditional computation, knowledge distillation, efficient fine-tuning techniques) will be crucial to reduce the computational and environmental footprint of large models. Exploring alternative computing paradigms and hardware co-design tailored for LLM training could also offer significant gains.\n",
       "\n",
       "5.  **Scaling for Alignment, Safety, and Robustness:**\n",
       "    A key future direction is to integrate ethical considerations, safety, and human alignment directly into the scaling process. This involves developing methods to measure and improve these attributes as models scale, rather than treating them as post-hoc adjustments. Research into scaling laws for robustness against adversarial attacks and for mitigating biases will be vital for building trustworthy AI systems.\n",
       "\n",
       "By addressing these challenges and pursuing these future directions, the field can move towards a more comprehensive, interpretable, efficient, and responsible understanding and application of LLM scaling laws.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The exploration of Large Language Model (LLM) scaling laws has emerged as a cornerstone in understanding and advancing the capabilities of artificial intelligence. These laws reveal a remarkably consistent and predictable relationship between computational resources (compute), dataset size, and model parameters, and the resulting model performance. Crucially, they demonstrate that performance improvements are not merely incremental but often follow a log-linear trajectory, indicating that significant gains are achievable by scaling up these fundamental inputs. The concept of 'optimal scaling,' exemplified by works like Chinchilla, further refined this understanding, highlighting the critical importance of balancing compute, data, and model size for maximum efficiency and performance. Perhaps most profoundly, scaling has been shown to unlock 'emergent abilities' â€“ capabilities that are not present in smaller models but spontaneously appear at larger scales, hinting at complex internal representations.\n",
       "\n",
       "The implications of these findings for the advancement of artificial intelligence are profound and multifaceted. Firstly, scaling laws provide a vital roadmap for researchers and developers, guiding resource allocation and strategic investment in model development. They enable more efficient training, reducing the trial-and-error inherent in previous AI development paradigms and accelerating the pace of innovation. Secondly, the predictability offered by these laws allows for more accurate forecasting of future AI capabilities, informing policy, ethical considerations, and long-term research agendas. Finally, the consistent demonstration that increased scale leads to enhanced performance and novel capabilities underscores the potential for ever more powerful and versatile AI systems, pushing the boundaries of what is computationally possible.\n",
       "\n",
       "In essence, LLM scaling laws are not just empirical observations; they are fundamental principles that underpin the current revolution in AI. They provide a scientific basis for understanding how intelligence emerges in large neural networks and offer a clear path towards building increasingly sophisticated and general-purpose AI. As we continue to explore the limits of scale, these laws will remain indispensable, charting the course for the next generation of artificial intelligence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(state[\"final_report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
